{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb93bd53",
   "metadata": {},
   "source": [
    "## Mini Project 03- NLP Emotions: Model Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1411eed",
   "metadata": {},
   "source": [
    "Dataset:\n",
    "- A. Tripathi, \"Emotion Classification NLP\", Kaggle.com, 2021. [Online]. Available: https://www.kaggle.com/datasets/anjaneyatripathi/emotion-classification-nlp. [Accessed: 16- Jul- 2022]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c767ff57",
   "metadata": {},
   "source": [
    "Sources:\n",
    "- WASSA-2017 Shared Task on Emotion Intensity. Saif M. Mohammad and Felipe Bravo-Marquez. In Proceedings of the EMNLP 2017 Workshop on Computational Approaches to Subjectivity, Sentiment, and Social Media (WASSA), September 2017, Copenhagen, Denmark.\n",
    "BibTex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be486c7f",
   "metadata": {},
   "source": [
    "Emotion Labels:\n",
    "- joy: 1\n",
    "- sadness: 2\n",
    "- anger: 3\n",
    "- fear: 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a87816",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "04fa357a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re as regex\n",
    "import spacy\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from simple_colors import *\n",
    "\n",
    "import string\n",
    "from collections import Counter\n",
    "import re as regex\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "import neattext.functions as nfx\n",
    "import nltk\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import import_ipynb\n",
    "import nlpEmotion_functions\n",
    "from nlpEmotion_functions import show_summary_report, calcRocPrecisionRecall, testCalcRocPrecisionRecall, calcRocAuc\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6832473",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "94009df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfData = pd.read_csv(\"convertedTextDataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "2befdba2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>short</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Just got back from seeing @GaryDelaney in Burs...</td>\n",
       "      <td>1</td>\n",
       "      <td>get see burslem amaze face hurt laugh hilarious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Oh dear an evening of absolute hilarity I don'...</td>\n",
       "      <td>1</td>\n",
       "      <td>dear even absolute hilarity don think laugh lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Been waiting all week for this game ❤️❤️❤️ #ch...</td>\n",
       "      <td>1</td>\n",
       "      <td>wait week game red heart red heart red heart c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>@gardiner_love : Thank you so much, Gloria! Yo...</td>\n",
       "      <td>1</td>\n",
       "      <td>thank gloria sweet thoughtful day joyful love ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>I feel so blessed to work with the family that...</td>\n",
       "      <td>1</td>\n",
       "      <td>feel bless work family nanny red heart love am...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  label  \\\n",
       "0           0  Just got back from seeing @GaryDelaney in Burs...      1   \n",
       "1           1  Oh dear an evening of absolute hilarity I don'...      1   \n",
       "2           2  Been waiting all week for this game ❤️❤️❤️ #ch...      1   \n",
       "3           3  @gardiner_love : Thank you so much, Gloria! Yo...      1   \n",
       "4           4  I feel so blessed to work with the family that...      1   \n",
       "\n",
       "                                               short  \n",
       "0    get see burslem amaze face hurt laugh hilarious  \n",
       "1  dear even absolute hilarity don think laugh lo...  \n",
       "2  wait week game red heart red heart red heart c...  \n",
       "3  thank gloria sweet thoughtful day joyful love ...  \n",
       "4  feel bless work family nanny red heart love am...  "
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "78743036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7102 entries, 0 to 7101\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   Unnamed: 0  7102 non-null   int64 \n",
      " 1   text        7102 non-null   object\n",
      " 2   label       7102 non-null   int64 \n",
      " 3   short       7097 non-null   object\n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 222.1+ KB\n"
     ]
    }
   ],
   "source": [
    "dfData.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "f3d5c6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfData.drop(['Unnamed: 0'], axis= 1, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "06ea75b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7102, 3)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfData.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecfba9f",
   "metadata": {},
   "source": [
    "### Split the data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "898e5cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## split the dataset\n",
    "# Features and Labels\n",
    "X= dfData['short']\n",
    "y= dfData['label']\n",
    "\n",
    "# split the dataset\n",
    "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size= 0.2, random_state= 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9560268",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ec8053",
   "metadata": {},
   "source": [
    "### Count Vectors as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "5aafd48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a count vectorizer object\n",
    "count_vect = CountVectorizer(token_pattern = r'\\w{1,}')\n",
    "\n",
    "# Learn a vocabulary dictionary of all tokens in the raw documents\n",
    "# count_vect.fit(dfData['text'])\n",
    "\n",
    "# Transform documents to document-term matrix.\n",
    "X_train_count = count_vect.fit_transform(X_train.astype('str'))\n",
    "X_test_count = count_vect.transform(X_test.astype('str'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "884d8206",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5681, 8917)"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_count.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "fa876f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a count vectorizer object\n",
    "# count_vect2 = CountVectorizer(token_pattern = r'\\w{1,}')\n",
    "\n",
    "# Learn a vocabulary dictionary of all tokens in the raw documents\n",
    "# count_vect.fit(dfData['text'])\n",
    "\n",
    "# Transform documents to document-term matrix.\n",
    "# X_train_count2 = count_vect2.fit_transform(X2_train.astype('str'))\n",
    "# X_test_count2 = count_vect2.transform(X2_test.astype('str'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b76c23e",
   "metadata": {},
   "source": [
    "### TF-IDF Vectors as features\n",
    "- Word level\n",
    "- N-Gram level\n",
    "- Character level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "4c9913d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer(max_features=5000, token_pattern='\\\\w{1,}')\n",
      "CPU times: total: 62.5 ms\n",
      "Wall time: 68.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer = 'word',\n",
    "                             token_pattern = r'\\w{1,}',\n",
    "                             max_features = 5000)\n",
    "print(tfidf_vect)\n",
    "\n",
    "# tfidf_vect.fit(dfData['text'])\n",
    "X_train_tfidf = tfidf_vect.fit_transform(X_train.astype('str'))\n",
    "X_test_tfidf  = tfidf_vect.transform(X_test.astype('str'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "abd3e9ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5681x5000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 39110 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "a7ce9468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer(max_features=5000, ngram_range=(2, 3), token_pattern='\\\\w{1,}')\n",
      "CPU times: total: 219 ms\n",
      "Wall time: 227 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# ngram level tf-idf\n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer = 'word',\n",
    "                                   token_pattern = r'\\w{1,}',\n",
    "                                   ngram_range = (2, 3),\n",
    "                                   max_features = 5000)\n",
    "print(tfidf_vect_ngram)\n",
    "\n",
    "# tfidf_vect_ngram.fit(dfData['text'])\n",
    "X_train_tfidf_ngram = tfidf_vect_ngram.fit_transform(X_train.astype('str'))\n",
    "X_test_tfidf_ngram  = tfidf_vect_ngram.transform(X_test.astype('str'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "e381edcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer(analyzer='char', max_features=5000, ngram_range=(2, 3))\n",
      "CPU times: total: 422 ms\n",
      "Wall time: 422 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# characters level tf-idf\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer = 'char',\n",
    "#                                          token_pattern = r'\\w{1,}',\n",
    "                                         ngram_range = (2, 3),\n",
    "                                         max_features = 5000)\n",
    "print(tfidf_vect_ngram_chars)\n",
    "\n",
    "# tfidf_vect_ngram_chars.fit(dfData['text'])\n",
    "X_train_tfidf_ngram_chars = tfidf_vect_ngram_chars.fit_transform(X_train.astype('str'))\n",
    "X_test_tfidf_ngram_chars  = tfidf_vect_ngram_chars.transform(X_test.astype('str'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60a94b1",
   "metadata": {},
   "source": [
    "### Text / NLP based features\n",
    "\n",
    "Create some other features.\n",
    "\n",
    "Char_Count = Number of Characters in Text\n",
    "\n",
    "Word Count = Number of Words in Text\n",
    "\n",
    "Word Density = Average Number of Char in Words\n",
    "\n",
    "Punctuation Count = Number of Punctuation in Text\n",
    "\n",
    "Title Word Count = Number of Words in Title\n",
    "\n",
    "Uppercase Word Count = Number of Upperwords in Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "6824eb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of characters in text\n",
    "def char_count(text):\n",
    "    return len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "135c2f44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hope bsnl slightly smile face speed slightly smile face win thank elate like neutral face slightly smile face'"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demoText= dfData['short'][4321]\n",
    "demoText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "8e4a0232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109\n"
     ]
    }
   ],
   "source": [
    "charCount= char_count(demoText)\n",
    "print(charCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "4354df7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of words in text\n",
    "def word_count(text):\n",
    "    return len(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "acf74627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "wordCount= word_count(demoText)\n",
    "print(wordCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "18f18bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average number of characters in words\n",
    "def word_density(text):\n",
    "    charCount= len(text)\n",
    "    wordCount= len(text.split())\n",
    "    wordDensity= charCount/ wordCount\n",
    "    return wordDensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "28af2da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.055555555555555\n"
     ]
    }
   ],
   "source": [
    "wordDensity= word_density(demoText)\n",
    "print(wordDensity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "761ea43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of punctuations in text\n",
    "def punctuation_count(text):\n",
    "    punctuations= \"!#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\n",
    "    d= dict()\n",
    "    for i in punctuations:\n",
    "        d[str(i)+' count']= text.count(i)\n",
    "    return d "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "ea61363e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'! count': 0, '# count': 0, '$ count': 0, '% count': 0, '& count': 0, \"' count\": 0, '( count': 0, ') count': 0, '* count': 0, '+ count': 0, ', count': 0, '- count': 0, '. count': 0, '/ count': 0, ': count': 0, '; count': 0, '< count': 0, '= count': 0, '> count': 0, '? count': 0, '@ count': 0, '[ count': 0, '\\\\ count': 0, '] count': 0, '^ count': 0, '_ count': 0, '` count': 0, '{ count': 0, '| count': 0, '} count': 0, '~ count': 0}\n"
     ]
    }
   ],
   "source": [
    "punctCount= punctuation_count(demoText)\n",
    "print(punctCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "e141ca76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of uppercase words in text\n",
    "def count_capital_words(text):\n",
    "    return sum(map(str.isupper, text.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "278d0d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "capitalWordCount= count_capital_words(demoText)\n",
    "print(capitalWordCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "8f9a1022",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load spaCy\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "10d9cf47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['slightly', 'ADV'], ['slightly', 'ADV'], ['slightly', 'ADV']]"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenisation\n",
    "# doc = nlp('Drinking a glass of wine is good for your wellbeing!')\n",
    "doc = nlp(demoText)\n",
    "doc_Adv= []\n",
    "for token in doc:\n",
    "#     print(f\"token:{token}\\t tag:{token.tag_}\\t\\tPOS:{token.pos_}\\t\\t text:'{token.text}' \\tlemma:{token.lemma_}\\t \")\n",
    "    if token.pos_ == 'ADV':\n",
    "        doc_Adv.append([token.text, token.pos_])\n",
    "doc_Adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "04860d64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "advFreq= len(doc_Adv)\n",
    "advFreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "0d4f65e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adj_count(cell):\n",
    "    doc = nlp(cell)\n",
    "    doc_Pos= []\n",
    "    for token in doc:\n",
    "#         print(f\"token:{token}\\t tag:{token.tag_}\\t\\tPOS:{token.pos_}\\t\\t text:'{token.text}' \\tlemma:{token.lemma_}\\t \")\n",
    "        if token.pos_ == 'ADJ':\n",
    "            doc_Pos.append([token.text, token.pos_])\n",
    "    posFreq= len(doc_Pos)\n",
    "    return posFreq\n",
    "\n",
    "def adv_count(cell):\n",
    "    doc = nlp(cell)\n",
    "    doc_Pos= []\n",
    "    for token in doc:\n",
    "#         print(f\"token:{token}\\t tag:{token.tag_}\\t\\tPOS:{token.pos_}\\t\\t text:'{token.text}' \\tlemma:{token.lemma_}\\t \")\n",
    "        if token.pos_ == 'ADV':\n",
    "            doc_Pos.append([token.text, token.pos_])\n",
    "    posFreq= len(doc_Pos)\n",
    "    return posFreq\n",
    "\n",
    "def noun_count(cell):\n",
    "    doc = nlp(cell)\n",
    "    doc_Pos= []\n",
    "    for token in doc:\n",
    "#         print(f\"token:{token}\\t tag:{token.tag_}\\t\\tPOS:{token.pos_}\\t\\t text:'{token.text}' \\tlemma:{token.lemma_}\\t \")\n",
    "        if token.pos_ == 'ADV':\n",
    "            doc_Pos.append([token.text, token.pos_])\n",
    "    posFreq= len(doc_Pos)\n",
    "    return posFreq\n",
    "\n",
    "def num_count(cell):\n",
    "    doc = nlp(cell)\n",
    "    doc_Pos= []\n",
    "    for token in doc:\n",
    "#         print(f\"token:{token}\\t tag:{token.tag_}\\t\\tPOS:{token.pos_}\\t\\t text:'{token.text}' \\tlemma:{token.lemma_}\\t \")\n",
    "        if token.pos_ == 'NUM':\n",
    "            doc_Pos.append([token.text, token.pos_])\n",
    "    posFreq= len(doc_Pos)\n",
    "    return posFreq\n",
    "\n",
    "def pron_count(cell):\n",
    "    doc = nlp(cell)\n",
    "    doc_Pos= []\n",
    "    for token in doc:\n",
    "#         print(f\"token:{token}\\t tag:{token.tag_}\\t\\tPOS:{token.pos_}\\t\\t text:'{token.text}' \\tlemma:{token.lemma_}\\t \")\n",
    "        if token.pos_ == 'PRON':\n",
    "            doc_Pos.append([token.text, token.pos_])\n",
    "    posFreq= len(doc_Pos)\n",
    "    return posFreq\n",
    "\n",
    "def propn_count(cell):\n",
    "    doc = nlp(cell)\n",
    "    doc_Pos= []\n",
    "    for token in doc:\n",
    "#         print(f\"token:{token}\\t tag:{token.tag_}\\t\\tPOS:{token.pos_}\\t\\t text:'{token.text}' \\tlemma:{token.lemma_}\\t \")\n",
    "        if token.pos_ == 'PROPN':\n",
    "            doc_Pos.append([token.text, token.pos_])\n",
    "    posFreq= len(doc_Pos)\n",
    "    return posFreq\n",
    "\n",
    "def verb_count(cell):\n",
    "    doc = nlp(cell)\n",
    "    doc_Pos= []\n",
    "    for token in doc:\n",
    "#         print(f\"token:{token}\\t tag:{token.tag_}\\t\\tPOS:{token.pos_}\\t\\t text:'{token.text}' \\tlemma:{token.lemma_}\\t \")\n",
    "        if token.pos_ == 'VERB':\n",
    "            doc_Pos.append([token.text, token.pos_])\n",
    "    posFreq= len(doc_Pos)\n",
    "    return posFreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "aae61462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # Initialise some columns for feature's counts\n",
    "# dfData['char_count']= dfData['text'].apply(lambda x: char_count(x))\n",
    "# dfData['word_count']= dfData['text'].apply(lambda x: word_count(x))\n",
    "# dfData['word_density']= dfData['text'].apply(lambda x: word_density(x))\n",
    "# # dfData['punctuation_count']= dfData['text'].apply(lambda x: punctuation_count(x))\n",
    "# # dfData['uppercase_word_count']= dfData['text'].apply(lambda x: count_capital_words(x))\n",
    "# dfData['adj_count']= dfData['text'].apply(lambda x: adj_count(x))\n",
    "# dfData['adv_count']= dfData['text'].apply(lambda x: adv_count(x))\n",
    "# dfData['noun_count']= dfData['text'].apply(lambda x: noun_count(x))\n",
    "# dfData['num_count']= dfData['text'].apply(lambda x: num_count(x))\n",
    "# dfData['pron_count']= dfData['text'].apply(lambda x: pron_count(x))\n",
    "# dfData['propn_count']= dfData['text'].apply(lambda x: propn_count(x))\n",
    "# dfData['verb_count']= dfData['text'].apply(lambda x: verb_count(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "fb7f7c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfData.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "a8a66593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepath = Path('nlpTrainFeatures.csv')  \n",
    "# filepath.parent.mkdir(parents=True, exist_ok=True)  \n",
    "# dfData.to_csv(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaaef9b9",
   "metadata": {},
   "source": [
    "### Topic Models as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "99f815ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 16.3 s\n",
      "Wall time: 16.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# train a LDA Model\n",
    "lda_model = LatentDirichletAllocation(n_components = 20, learning_method = 'online', max_iter = 20)\n",
    "\n",
    "Xtrain_topics = lda_model.fit_transform(X_train_count)\n",
    "Xtest_topics = lda_model.transform(X_test_count)\n",
    "topic_word = lda_model.components_ \n",
    "vocab = count_vect.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "ab1b6ef6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5681, 20)"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain_topics.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "cc101a85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1421, 20)"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtest_topics.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "a46b5e53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group Top Words\n",
      "----- --------------------------------------------------------------------------------\n",
      "    0 heart god game follow red party damn joyous awe sick\n",
      "    1 face joy tear lose say horror shake break fan turn\n",
      "    2 get anxiety depression tonight revenge new fume thank give great\n",
      "    3 think shock hold long head madden evil bright reason breezy\n",
      "    4 fuck feel right have like despair world help sleep actually\n",
      "    5 face smile start eye cry nervous weary loudly hope sober\n",
      "    6 people talk worry terrorism way skin person tone light dark\n",
      "    7 anger stop try year doesn wish end delight hilarious month\n",
      "    8 love like look work make sad pout little rage laugh\n",
      "    9 angry day didn away snap post class fall birthday order\n",
      "   10 want find unhappy hour chirp success car wow dog keep\n",
      "   11 blue life well gbbo job meet family cause beautiful care\n",
      "   12 good need fear cheer frown point day quote morning rejoice\n",
      "   13 time play awful sadness best stay gon guy player sorry\n",
      "   14 amp sparkle laughter like fire die wanna run burn half\n",
      "   15 hand lol man leave hate fun funny read moment true\n",
      "   16 watch bad live today afraid amaze dreadful lively horrible use\n",
      "   17 thing week optimism big bully terrible shit bitter remember get\n",
      "   18 don know let panic believe rag mind change pay goal\n",
      "   19 happy offend mean team ask friend isn hear hilarity yeah\n"
     ]
    }
   ],
   "source": [
    "# view the topic models\n",
    "n_top_words = 10\n",
    "topic_summaries = []\n",
    "print('Group Top Words')\n",
    "print('-----', '-'*80)\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    top_words = ' '.join(topic_words)\n",
    "    topic_summaries.append(top_words)\n",
    "    print('  %3d %s' % (i, top_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "1930fec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTrainTopic= pd.DataFrame(Xtrain_topics)\n",
    "dfTestTopic= pd.DataFrame(Xtest_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "0bb4891b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5681, 20)"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfTrainTopic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "b0532aed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1421, 20)"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfTestTopic.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6b9f66",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "e73c7d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## helper function\n",
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid, testLabel):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "\n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    accScore= accuracy_score(predictions, testLabel)\n",
    "    return [accScore, predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "0116bd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep the results in a dataframe\n",
    "results = pd.DataFrame(columns = ['Count Vectors',\n",
    "                                  'WordLevel TF-IDF',\n",
    "                                  'N-Gram Vectors',\n",
    "                                  'CharLevel Vectors'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bc662e",
   "metadata": {},
   "source": [
    "### Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "e934698e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, Count Vectors    : 0.8304\n",
      "\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 3.99 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Naive Bayes on Count Vectors\n",
    "gnbAccuracy1 = train_model(MultinomialNB(), X_train_count, y_train, X_test_count, y_test)\n",
    "print('NB, Count Vectors    : %.4f\\n' % gnbAccuracy1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "d7931f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, WordLevel TF-IDF : 0.8086\n",
      "\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 2.99 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Naive Bayes on Word Level TF IDF Vectors\n",
    "gnbAccuracy2 = train_model(MultinomialNB(), X_train_tfidf, y_train, X_test_tfidf, y_test)\n",
    "print('NB, WordLevel TF-IDF : %.4f\\n' % gnbAccuracy2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "979db07e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, N-Gram Vectors   : 0.3955\n",
      "\n",
      "CPU times: total: 15.6 ms\n",
      "Wall time: 2.99 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Naive Bayes on Ngram Level TF IDF Vectors\n",
    "gnbAccuracy3 = train_model(MultinomialNB(), X_train_tfidf_ngram, y_train, X_test_tfidf_ngram, y_test)\n",
    "print('NB, N-Gram Vectors   : %.4f\\n' % gnbAccuracy3[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "476688d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, CharLevel Vectors: 0.6953\n",
      "\n",
      "CPU times: total: 31.2 ms\n",
      "Wall time: 5.98 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# # Naive Bayes on Character Level TF IDF Vectors\n",
    "gnbAccuracy4 = train_model(MultinomialNB(), X_train_tfidf_ngram_chars, y_train, X_test_tfidf_ngram_chars, y_test)\n",
    "print('NB, CharLevel Vectors: %.4f\\n' % gnbAccuracy4[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "ddf1b94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.loc['Naïve Bayes'] = {\n",
    "    'Count Vectors': gnbAccuracy1[0],\n",
    "    'WordLevel TF-IDF': gnbAccuracy2[0],\n",
    "    'N-Gram Vectors': gnbAccuracy3[0],\n",
    "    'CharLevel Vectors': gnbAccuracy4[0]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353f4dc0",
   "metadata": {},
   "source": [
    "### Linear Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "e7ed6a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, Count Vectors    : 0.8431\n",
      "\n",
      "CPU times: total: 859 ms\n",
      "Wall time: 1.11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Linear Classifier on Count Vectors\n",
    "logAccuracy1 = train_model(LogisticRegression(solver = 'lbfgs', max_iter = 350), X_train_count, y_train, X_test_count, y_test)\n",
    "print('LR, Count Vectors    : %.4f\\n' % logAccuracy1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "b9e706b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, WordLevel TF-IDF : 0.8480\n",
      "\n",
      "CPU times: total: 547 ms\n",
      "Wall time: 587 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Linear Classifier on Word Level TF IDF Vectors\n",
    "logAccuracy2 = train_model(LogisticRegression(solver = 'lbfgs', max_iter = 100), X_train_tfidf, y_train, X_test_tfidf, y_test)\n",
    "print('LR, WordLevel TF-IDF : %.4f\\n' % logAccuracy2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "1184bbc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, N-Gram Vectors   : 0.3955\n",
      "\n",
      "CPU times: total: 562 ms\n",
      "Wall time: 512 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Linear Classifier on Ngram Level TF IDF Vectors\n",
    "logAccuracy3 = train_model(LogisticRegression(solver = 'lbfgs', max_iter = 100), X_train_tfidf_ngram, y_train,\n",
    "                           X_test_tfidf_ngram, y_test)\n",
    "print('LR, N-Gram Vectors   : %.4f\\n' % logAccuracy3[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "dcb6b265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, CharLevel Vectors: 0.7797\n",
      "\n",
      "CPU times: total: 891 ms\n",
      "Wall time: 1.04 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Linear Classifier on Character Level TF IDF Vectors\n",
    "logAccuracy4 = train_model(LogisticRegression(solver = 'lbfgs', max_iter = 100), X_train_tfidf_ngram_chars, y_train,\n",
    "                           X_test_tfidf_ngram_chars, y_test)\n",
    "print('LR, CharLevel Vectors: %.4f\\n' % logAccuracy4[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "6bf222e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.loc['Logistic Regression'] = {\n",
    "    'Count Vectors': logAccuracy1[0],\n",
    "    'WordLevel TF-IDF': logAccuracy2[0],\n",
    "    'N-Gram Vectors': logAccuracy3[0],\n",
    "    'CharLevel Vectors': logAccuracy4[0]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b52edd",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "8ec17861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM, Count Vectors    : 0.8360\n",
      "\n",
      "CPU times: total: 422 ms\n",
      "Wall time: 422 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Support Vector Machine on Count Vectors\n",
    "svmAccuracy1 = train_model(LinearSVC(), X_train_count, y_train, X_test_count, y_test)\n",
    "print('SVM, Count Vectors    : %.4f\\n' % svmAccuracy1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "70df9015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM, WordLevel TF-IDF : 0.8550\n",
      "\n",
      "CPU times: total: 46.9 ms\n",
      "Wall time: 41.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Support Vector Machine on Word Level TF IDF Vectors\n",
    "svmAccuracy2 = train_model(LinearSVC(), X_train_tfidf, y_train, X_test_tfidf, y_test)\n",
    "print('SVM, WordLevel TF-IDF : %.4f\\n' % svmAccuracy2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "30edc177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM, N-Gram Vectors   : 0.3885\n",
      "\n",
      "CPU times: total: 62.5 ms\n",
      "Wall time: 47.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Support Vector Machine on Ngram Level TF IDF Vectors\n",
    "svmAccuracy3 = train_model(LinearSVC(), X_train_tfidf_ngram, y_train, X_test_tfidf_ngram, y_test)\n",
    "print('SVM, N-Gram Vectors   : %.4f\\n' % svmAccuracy3[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "14c6c95c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM, CharLevel Vectors: 0.8030\n",
      "\n",
      "CPU times: total: 203 ms\n",
      "Wall time: 205 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Support Vector Machine on Character Level TF IDF Vectors\n",
    "svmAccuracy4 = train_model(LinearSVC(), X_train_tfidf_ngram_chars, y_train, X_test_tfidf_ngram_chars, y_test)\n",
    "print('SVM, CharLevel Vectors: %.4f\\n' % svmAccuracy4[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "0421f594",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.loc['Support Vector Machine'] = {\n",
    "    'Count Vectors': svmAccuracy1[0],\n",
    "    'WordLevel TF-IDF': svmAccuracy2[0],\n",
    "    'N-Gram Vectors': svmAccuracy3[0],\n",
    "    'CharLevel Vectors': svmAccuracy4[0]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeebb4af",
   "metadata": {},
   "source": [
    "### Bagging Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "9959a176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF, Count Vectors    : 0.8318\n",
      "\n",
      "CPU times: total: 5.84 s\n",
      "Wall time: 5.84 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Bagging (Random Forest) on Count Vectors\n",
    "rfcAccuracy1 = train_model(RandomForestClassifier(n_estimators = 100), X_train_count, y_train, X_test_count, y_test)\n",
    "print('RF, Count Vectors    : %.4f\\n' % rfcAccuracy1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "dea58133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF, WordLevel TF-IDF : 0.8234\n",
      "\n",
      "CPU times: total: 3.98 s\n",
      "Wall time: 3.99 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Bagging (Random Forest) on Word Level TF IDF Vectors\n",
    "rfcAccuracy2 = train_model(RandomForestClassifier(n_estimators = 100), X_train_tfidf, y_train, X_test_tfidf, y_test)\n",
    "print('RF, WordLevel TF-IDF : %.4f\\n' % rfcAccuracy2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "c6b7d67b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF, N-Gram Vectors   : 0.3871\n",
      "\n",
      "CPU times: total: 4.66 s\n",
      "Wall time: 4.66 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Bagging (Random Forest) on Ngram Level TF IDF Vectors\n",
    "rfcAccuracy3 = train_model(RandomForestClassifier(n_estimators = 100), X_train_tfidf_ngram, y_train, X_test_tfidf_ngram, y_test)\n",
    "print('RF, N-Gram Vectors   : %.4f\\n' % rfcAccuracy3[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "496af0fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF, CharLevel Vectors: 0.7628\n",
      "\n",
      "CPU times: total: 11.1 s\n",
      "Wall time: 11.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Bagging (Random Forest) on Character Level TF IDF Vectors\n",
    "rfcAccuracy4 = train_model(RandomForestClassifier(n_estimators = 100), X_train_tfidf_ngram_chars, y_train,\n",
    "                           X_test_tfidf_ngram_chars, y_test)\n",
    "print('RF, CharLevel Vectors: %.4f\\n' % rfcAccuracy4[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "978fc756",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.loc['Random Forest'] = {\n",
    "    'Count Vectors': rfcAccuracy1[0],\n",
    "    'WordLevel TF-IDF': rfcAccuracy2[0],\n",
    "    'N-Gram Vectors': rfcAccuracy3[0],\n",
    "    'CharLevel Vectors': rfcAccuracy4[0]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ae5400",
   "metadata": {},
   "source": [
    "### Boosting Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "8fcb249c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GB, Count Vectors    : 0.8128\n",
      "\n",
      "CPU times: total: 3.59 s\n",
      "Wall time: 3.59 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Gradient Boosting on Count Vectors\n",
    "gradBstAccuracy1 = train_model(GradientBoostingClassifier(), X_train_count, y_train, X_test_count, y_test)\n",
    "print('GB, Count Vectors    : %.4f\\n' % gradBstAccuracy1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "d63e818c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GB, WordLevel TF-IDF : 0.8100\n",
      "\n",
      "CPU times: total: 4.62 s\n",
      "Wall time: 4.63 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Gradient Boosting on Word Level TF IDF Vectors\n",
    "gradBstAccuracy2 = train_model(GradientBoostingClassifier(), X_train_tfidf, y_train, X_test_tfidf, y_test)\n",
    "print('GB, WordLevel TF-IDF : %.4f\\n' % gradBstAccuracy2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "e545af4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GB, N-Gram Vectors   : 0.3842\n",
      "\n",
      "CPU times: total: 2.11 s\n",
      "Wall time: 2.13 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Gradient Boosting on Ngram Level TF IDF Vectors\n",
    "gradBstAccuracy3 = train_model(GradientBoostingClassifier(), X_train_tfidf_ngram, y_train, X_test_tfidf_ngram, y_test)\n",
    "print('GB, N-Gram Vectors   : %.4f\\n' % gradBstAccuracy3[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "03d4a2f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GB, CharLevel Vectors: 0.7889\n",
      "\n",
      "CPU times: total: 51.6 s\n",
      "Wall time: 51.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Gradient Boosting on Character Level TF IDF Vectors\n",
    "gradBstAccuracy4 = train_model(GradientBoostingClassifier(), X_train_tfidf_ngram_chars, y_train, X_test_tfidf_ngram_chars,\n",
    "                              y_test)\n",
    "print('GB, CharLevel Vectors: %.4f\\n' % gradBstAccuracy4[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "3418dc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.loc['Gradient Boosting'] = {\n",
    "    'Count Vectors': gradBstAccuracy1[0],\n",
    "    'WordLevel TF-IDF': gradBstAccuracy2[0],\n",
    "    'N-Gram Vectors': gradBstAccuracy3[0],\n",
    "    'CharLevel Vectors': gradBstAccuracy4[0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "412a6066",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Count Vectors</th>\n",
       "      <th>WordLevel TF-IDF</th>\n",
       "      <th>N-Gram Vectors</th>\n",
       "      <th>CharLevel Vectors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Naïve Bayes</th>\n",
       "      <td>0.830401</td>\n",
       "      <td>0.808586</td>\n",
       "      <td>0.395496</td>\n",
       "      <td>0.695285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.843068</td>\n",
       "      <td>0.847994</td>\n",
       "      <td>0.395496</td>\n",
       "      <td>0.779733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Support Vector Machine</th>\n",
       "      <td>0.836031</td>\n",
       "      <td>0.855032</td>\n",
       "      <td>0.388459</td>\n",
       "      <td>0.802956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.831809</td>\n",
       "      <td>0.823364</td>\n",
       "      <td>0.387051</td>\n",
       "      <td>0.762843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boosting</th>\n",
       "      <td>0.812808</td>\n",
       "      <td>0.809993</td>\n",
       "      <td>0.384236</td>\n",
       "      <td>0.788881</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Count Vectors  WordLevel TF-IDF  N-Gram Vectors  \\\n",
       "Naïve Bayes                  0.830401          0.808586        0.395496   \n",
       "Logistic Regression          0.843068          0.847994        0.395496   \n",
       "Support Vector Machine       0.836031          0.855032        0.388459   \n",
       "Random Forest                0.831809          0.823364        0.387051   \n",
       "Gradient Boosting            0.812808          0.809993        0.384236   \n",
       "\n",
       "                        CharLevel Vectors  \n",
       "Naïve Bayes                      0.695285  \n",
       "Logistic Regression              0.779733  \n",
       "Support Vector Machine           0.802956  \n",
       "Random Forest                    0.762843  \n",
       "Gradient Boosting                0.788881  "
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "dc718028",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = Path('modelPhase01ResultSummary.csv')  \n",
    "filepath.parent.mkdir(parents=True, exist_ok=True)  \n",
    "results.to_csv(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a6ca66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
