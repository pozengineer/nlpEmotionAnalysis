{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4313582e",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24462181",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re as regex\n",
    "import spacy\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from simple_colors import *\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import string\n",
    "from collections import Counter\n",
    "import re as regex\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "import neattext.functions as nfx\n",
    "import nltk\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2657ddbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22030b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import cycle\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d94f8501",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_summary_report(actual, prediction, predictProba, modelTag, imageTag):\n",
    "\n",
    "    if isinstance(actual, pd.Series):\n",
    "        actual = actual.values\n",
    "    if actual.dtype.name == 'object':\n",
    "        actual = actual.astype(int)\n",
    "    if prediction.dtype.name == 'object':\n",
    "        prediction = prediction.astype(int)\n",
    "\n",
    "    accuracy_ = accuracy_score(actual, prediction)\n",
    "    precision_ = precision_score(actual, prediction,\n",
    "                                 pos_label='positive',\n",
    "                                 average='macro')\n",
    "    recall_ = recall_score(actual, prediction,\n",
    "                           pos_label='positive',\n",
    "                           average='macro')\n",
    "    f1_score_ = f1_score(actual, prediction,\n",
    "                           pos_label='positive',\n",
    "                           average='macro')\n",
    "    roc_auc_ = roc_auc_score(actual, predictProba,\n",
    "                             multi_class= 'ovr',\n",
    "                             average='macro')\n",
    "\n",
    "    print('Accuracy  : %.4f [TP / N] Proportion of predicted labels that match the true labels. Best: 1, Worst: 0' % accuracy_)\n",
    "    print('Precision : %.4f [TP / (TP + FP)] Not to label a negative sample as positive.        Best: 1, Worst: 0' % precision_)\n",
    "    print('Recall    : %.4f [TP / (TP + FN)] Find all the positive samples.                     Best: 1, Worst: 0' % recall_)\n",
    "    print('f1-score  : %.4f [2 * (Precision * Recall)/ (Precision + Recall)]                    Best: 1, Worst: 0' % f1_score_)\n",
    "    print('ROC AUC   : %.4f                                                                     Best: 1, Worst: < 0.5' % roc_auc_)\n",
    "    print('-' * 107)\n",
    "    print('TP: True Positives, FP: False Positives, TN: True Negatives, FN: False Negatives, N: Number of samples')\n",
    "\n",
    "    # Confusion Matrix\n",
    "    confuseMatrix = confusion_matrix(actual, prediction)\n",
    "    confuseLabels= ('joy', 'sadness', 'anger', 'fear')\n",
    "    # plot\n",
    "    fig, ax = plt.subplots(1, 1, figsize = (8, 6))\n",
    "    fig.subplots_adjust(left = 0.02, right = 0.98, wspace = 0.2)\n",
    "\n",
    "    # Confusion Matrix\n",
    "    sns.heatmap(confuseMatrix.T, square= True, annot= True, fmt= 'd', cbar= False, cmap= 'Blues', ax= ax,\n",
    "               xticklabels= confuseLabels, yticklabels= confuseLabels)\n",
    "\n",
    "    ax.set_title(f'Confusion Matrix ({modelTag})')\n",
    "    ax.set_xlabel('True label')\n",
    "    ax.set_ylabel('Predicted label')\n",
    "    \n",
    "    plt.savefig(f'{imageTag}.png', facecolor='w', bbox_inches=\"tight\",\n",
    "            pad_inches=0.3, transparent=True)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return [accuracy_, precision_, recall_, f1_score_, roc_auc_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32c417f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svcRocPrecisionRecall(X, y, model, vectorizer, modelTag, imageTag):\n",
    "    new_Y05 = label_binarize(y, classes=[1, 2, 3, 4])\n",
    "    n_classes = new_Y05.shape[1]\n",
    "    X_train05, X_test05, y_train05, y_test05 = train_test_split(X, new_Y05, test_size=0.2, random_state= 1)\n",
    "    counts05= vectorizer\n",
    "    A05= counts05.fit_transform(X_train05.values.astype('str'), y_train05)\n",
    "    B05= counts05.transform(X_test05.values.astype('str'))\n",
    "    classifier = OneVsRestClassifier(model)\n",
    "    classifier.fit(A05, y_train05)\n",
    "    y_score= classifier.predict(B05)\n",
    "#     predictProbDict= classifier.predict_proba(B05)\n",
    "    predictProbDict= classifier.decision_function(B05)\n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(n_classes):\n",
    "    #     print(i)\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_test05[:, i], predictProbDict[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    \n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test05.ravel(), predictProbDict.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "    \n",
    "    # First aggregate all false positive rates\n",
    "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "    # Then interpolate all ROC curves at this points\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "    for i in range(n_classes):\n",
    "        mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "    # Finally average it and compute AUC\n",
    "    mean_tpr /= n_classes\n",
    "\n",
    "    fpr[\"macro\"] = all_fpr\n",
    "    tpr[\"macro\"] = mean_tpr\n",
    "    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "    \n",
    "    # Compute precision and recall area for each class\n",
    "    precision= dict()\n",
    "    recall= dict()\n",
    "    average_precision = dict()\n",
    "    for i in range(n_classes):\n",
    "        precision[i], recall[i], _ = precision_recall_curve(y_test05[:, i], predictProbDict[:, i])\n",
    "        average_precision[i] = average_precision_score(y_test05[:, i], predictProbDict[:, i])\n",
    "\n",
    "    # A \"micro-average\": quantifying score on all classes jointly\n",
    "    precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(y_test05.ravel(), predictProbDict.ravel()\n",
    "    )\n",
    "    average_precision[\"micro\"] = average_precision_score(y_test05, predictProbDict, average=\"micro\")\n",
    "        \n",
    "    # plot\n",
    "    fig, ax= plt.subplots(1, 3, figsize= (18, 6))\n",
    "    fig.subplots_adjust(left= 0.02, right= 0.98, wspace= 0.2)\n",
    "    colors = cycle([\"aqua\", \"darkorange\", \"cornflowerblue\", \"green\"])\n",
    "    # Precision/Recall\n",
    "    f_scores = np.linspace(0.2, 0.8, num=4)\n",
    "    lines, labels = [], []\n",
    "    for f_score in f_scores:\n",
    "        x = np.linspace(0.01, 1)\n",
    "        y = f_score * x / (2 * x - f_score)\n",
    "        (l,) = ax[0].plot(x[y >= 0], y[y >= 0], color=\"gray\", alpha=0.2)\n",
    "        ax[0].annotate(\"f1={0:0.1f}\".format(f_score), xy=(0.9, y[45] + 0.02))\n",
    "\n",
    "    display = PrecisionRecallDisplay(\n",
    "        recall= recall[\"micro\"],\n",
    "        precision= precision[\"micro\"],\n",
    "        average_precision= average_precision[\"micro\"],\n",
    "    )\n",
    "    display.plot(ax= ax[0], name=\"Micro-average precision-recall\", color=\"deeppink\", linewidth= 1, linestyle=':')\n",
    "\n",
    "    for i, color in zip(range(n_classes), colors):\n",
    "        display = PrecisionRecallDisplay(\n",
    "            recall=recall[i],\n",
    "            precision=precision[i],\n",
    "            average_precision=average_precision[i],\n",
    "        )\n",
    "        display.plot(ax= ax[0], name= f\"Precision-recall for class {i+1}\", color= color, linewidth= 1)\n",
    "\n",
    "    # add the legend for the iso-f1 curves\n",
    "    handles, labels = display.ax_.get_legend_handles_labels()\n",
    "    handles.extend([l])\n",
    "    labels.extend([\"iso-f1 curves\"])\n",
    "    # set the legend and the axes\n",
    "    ax[0].set_xlim([0.0, 1.0])\n",
    "    ax[0].set_ylim([0.0, 1.05])\n",
    "    ax[0].legend(handles=handles, labels=labels, loc=\"best\")\n",
    "    ax[0].set_title(f'Extension of Precision-Recall curve to multi-class ({modelTag})')\n",
    "    \n",
    "#     Plot all ROC curves\n",
    "#     plt.figure(figsize=(20, 8))\n",
    "#     plt.figure(figsize=(10, 7))\n",
    "#     plt.subplot(121)\n",
    "    ax[1].plot(\n",
    "        fpr[\"micro\"],\n",
    "        tpr[\"micro\"],\n",
    "        label=\"micro-average ROC curve (area = {0:0.2f})\".format(roc_auc[\"micro\"]),\n",
    "        color=\"deeppink\",\n",
    "        linestyle=\":\",\n",
    "        linewidth=2,\n",
    "    )\n",
    "\n",
    "    ax[1].plot(\n",
    "        fpr[\"macro\"],\n",
    "        tpr[\"macro\"],\n",
    "        label=\"macro-average ROC curve (area = {0:0.2f})\".format(roc_auc[\"macro\"]),\n",
    "        color=\"navy\",\n",
    "        linestyle=\":\",\n",
    "        linewidth=2,\n",
    "    )\n",
    "    \n",
    "    lw= 1\n",
    "#     colors = cycle([\"aqua\", \"darkorange\", \"cornflowerblue\", \"green\"])\n",
    "    for i, color in zip(range(n_classes), colors):\n",
    "        ax[1].plot(\n",
    "            fpr[i],\n",
    "            tpr[i],\n",
    "            color=color,\n",
    "            lw= lw,\n",
    "            label=\"ROC curve of class {0} (area = {1:0.2f})\".format(i+1, roc_auc[i]),\n",
    "        )\n",
    "    \n",
    "    ax[1].plot([0, 1], [0, 1], \"k--\", lw= lw)\n",
    "    ax[1].set_xlim([-0.05, 1.0])\n",
    "    ax[1].set_ylim([0.0, 1.05])\n",
    "    ax[1].set_xlabel(\"False Positive Rate\")\n",
    "    ax[1].set_ylabel(\"True Positive Rate\")\n",
    "    ax[1].set_title(f'Some extension of Receiver operating characteristic to multiclass ({modelTag})')\n",
    "    ax[1].legend(loc=\"lower right\")\n",
    "    \n",
    "#     plt.subplot(122)\n",
    "    ax[2].plot(\n",
    "        fpr[\"micro\"],\n",
    "        tpr[\"micro\"],\n",
    "        label=\"micro-average ROC curve (area = {0:0.2f})\".format(roc_auc[\"micro\"]),\n",
    "        color=\"deeppink\",\n",
    "        linestyle=\":\",\n",
    "        linewidth=2,\n",
    "    )\n",
    "\n",
    "    ax[2].plot(\n",
    "        fpr[\"macro\"],\n",
    "        tpr[\"macro\"],\n",
    "        label=\"macro-average ROC curve (area = {0:0.2f})\".format(roc_auc[\"macro\"]),\n",
    "        color=\"navy\",\n",
    "        linestyle=\":\",\n",
    "        linewidth=2,\n",
    "    )\n",
    "    \n",
    "    lw= 1\n",
    "#     colors = cycle([\"aqua\", \"darkorange\", \"cornflowerblue\", \"green\"])\n",
    "    for i, color in zip(range(n_classes), colors):\n",
    "        ax[2].plot(\n",
    "            fpr[i],\n",
    "            tpr[i],\n",
    "            color=color,\n",
    "            lw= lw,\n",
    "            label=\"ROC curve of class {0} (area = {1:0.2f})\".format(i+1, roc_auc[i]),\n",
    "        )\n",
    "    \n",
    "    ax[2].plot([0, 1], [0, 1], \"k--\", lw= lw)\n",
    "    ax[2].set_xlim([-0.05, 0.5])\n",
    "    ax[2].set_ylim([0.5, 1.02])\n",
    "    ax[2].set_xlabel(\"False Positive Rate\")\n",
    "    ax[2].set_ylabel(\"True Positive Rate\")\n",
    "    ax[2].set_title(f'Magnified Roc-Curve ({modelTag})')\n",
    "    ax[2].legend(loc=\"lower right\")\n",
    "    \n",
    "    plt.savefig(f'{imageTag}.png', facecolor='w', bbox_inches=\"tight\",\n",
    "            pad_inches=0.3, transparent=True)\n",
    "    \n",
    "    plt.show()\n",
    "    return [n_classes, fpr, tpr, roc_auc, fpr['micro'], tpr['micro'], roc_auc['micro'],\n",
    "            fpr['macro'], tpr['macro'], roc_auc['macro'], precision[\"micro\"], recall[\"micro\"],\n",
    "            average_precision[\"micro\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "415ab68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def probaRocPrecisionRecall(X, y, model, vectorizer, modelTag, imageTag):\n",
    "    new_Y05 = label_binarize(y, classes=[1, 2, 3, 4])\n",
    "    n_classes = new_Y05.shape[1]\n",
    "    X_train05, X_test05, y_train05, y_test05 = train_test_split(X, new_Y05, test_size=0.2, random_state= 1)\n",
    "    counts05= vectorizer\n",
    "    A05= counts05.fit_transform(X_train05.values.astype('str'), y_train05)\n",
    "    B05= counts05.transform(X_test05.values.astype('str'))\n",
    "    classifier = OneVsRestClassifier(model)\n",
    "    classifier.fit(A05, y_train05)\n",
    "    y_score= classifier.predict(B05)\n",
    "#     predictProbDict= classifier.predict_proba(B05)\n",
    "    predictProbDict= classifier.predict_proba(B05)\n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(n_classes):\n",
    "    #     print(i)\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_test05[:, i], predictProbDict[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    \n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test05.ravel(), predictProbDict.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "    \n",
    "    # First aggregate all false positive rates\n",
    "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "    # Then interpolate all ROC curves at this points\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "    for i in range(n_classes):\n",
    "        mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "    # Finally average it and compute AUC\n",
    "    mean_tpr /= n_classes\n",
    "\n",
    "    fpr[\"macro\"] = all_fpr\n",
    "    tpr[\"macro\"] = mean_tpr\n",
    "    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "    \n",
    "    # Compute precision and recall area for each class\n",
    "    precision= dict()\n",
    "    recall= dict()\n",
    "    average_precision = dict()\n",
    "    for i in range(n_classes):\n",
    "        precision[i], recall[i], _ = precision_recall_curve(y_test05[:, i], predictProbDict[:, i])\n",
    "        average_precision[i] = average_precision_score(y_test05[:, i], predictProbDict[:, i])\n",
    "\n",
    "    # A \"micro-average\": quantifying score on all classes jointly\n",
    "    precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(y_test05.ravel(), predictProbDict.ravel()\n",
    "    )\n",
    "    average_precision[\"micro\"] = average_precision_score(y_test05, predictProbDict, average=\"micro\")\n",
    "        \n",
    "    # plot\n",
    "    fig, ax= plt.subplots(1, 3, figsize= (18, 6))\n",
    "    fig.subplots_adjust(left= 0.02, right= 0.98, wspace= 0.2)\n",
    "    colors = cycle([\"aqua\", \"darkorange\", \"cornflowerblue\", \"green\"])\n",
    "    # Precision/Recall\n",
    "    f_scores = np.linspace(0.2, 0.8, num=4)\n",
    "    lines, labels = [], []\n",
    "    for f_score in f_scores:\n",
    "        x = np.linspace(0.01, 1)\n",
    "        y = f_score * x / (2 * x - f_score)\n",
    "        (l,) = ax[0].plot(x[y >= 0], y[y >= 0], color=\"gray\", alpha=0.2)\n",
    "        ax[0].annotate(\"f1={0:0.1f}\".format(f_score), xy=(0.9, y[45] + 0.02))\n",
    "\n",
    "    display = PrecisionRecallDisplay(\n",
    "        recall= recall[\"micro\"],\n",
    "        precision= precision[\"micro\"],\n",
    "        average_precision= average_precision[\"micro\"],\n",
    "    )\n",
    "    display.plot(ax= ax[0], name=\"Micro-average precision-recall\", color=\"deeppink\", linewidth= 1, linestyle=':')\n",
    "\n",
    "    for i, color in zip(range(n_classes), colors):\n",
    "        display = PrecisionRecallDisplay(\n",
    "            recall=recall[i],\n",
    "            precision=precision[i],\n",
    "            average_precision=average_precision[i],\n",
    "        )\n",
    "        display.plot(ax= ax[0], name= f\"Precision-recall for class {i+1}\", color= color, linewidth= 1)\n",
    "\n",
    "    # add the legend for the iso-f1 curves\n",
    "    handles, labels = display.ax_.get_legend_handles_labels()\n",
    "    handles.extend([l])\n",
    "    labels.extend([\"iso-f1 curves\"])\n",
    "    # set the legend and the axes\n",
    "    ax[0].set_xlim([0.0, 1.0])\n",
    "    ax[0].set_ylim([0.0, 1.05])\n",
    "    ax[0].legend(handles=handles, labels=labels, loc=\"best\")\n",
    "    ax[0].set_title(f'Extension of Precision-Recall curve to multi-class ({modelTag})')\n",
    "    \n",
    "#     Plot all ROC curves\n",
    "#     plt.figure(figsize=(20, 8))\n",
    "#     plt.figure(figsize=(10, 7))\n",
    "#     plt.subplot(121)\n",
    "    ax[1].plot(\n",
    "        fpr[\"micro\"],\n",
    "        tpr[\"micro\"],\n",
    "        label=\"micro-average ROC curve (area = {0:0.2f})\".format(roc_auc[\"micro\"]),\n",
    "        color=\"deeppink\",\n",
    "        linestyle=\":\",\n",
    "        linewidth=2,\n",
    "    )\n",
    "\n",
    "    ax[1].plot(\n",
    "        fpr[\"macro\"],\n",
    "        tpr[\"macro\"],\n",
    "        label=\"macro-average ROC curve (area = {0:0.2f})\".format(roc_auc[\"macro\"]),\n",
    "        color=\"navy\",\n",
    "        linestyle=\":\",\n",
    "        linewidth=2,\n",
    "    )\n",
    "    \n",
    "    lw= 1\n",
    "#     colors = cycle([\"aqua\", \"darkorange\", \"cornflowerblue\", \"green\"])\n",
    "    for i, color in zip(range(n_classes), colors):\n",
    "        ax[1].plot(\n",
    "            fpr[i],\n",
    "            tpr[i],\n",
    "            color=color,\n",
    "            lw= lw,\n",
    "            label=\"ROC curve of class {0} (area = {1:0.2f})\".format(i+1, roc_auc[i]),\n",
    "        )\n",
    "    \n",
    "    ax[1].plot([0, 1], [0, 1], \"k--\", lw= lw)\n",
    "    ax[1].set_xlim([-0.05, 1.0])\n",
    "    ax[1].set_ylim([0.0, 1.05])\n",
    "    ax[1].set_xlabel(\"False Positive Rate\")\n",
    "    ax[1].set_ylabel(\"True Positive Rate\")\n",
    "    ax[1].set_title(f'Some extension of Receiver operating characteristic to multiclass ({modelTag})')\n",
    "    ax[1].legend(loc=\"lower right\")\n",
    "    \n",
    "#     plt.subplot(122)\n",
    "    ax[2].plot(\n",
    "        fpr[\"micro\"],\n",
    "        tpr[\"micro\"],\n",
    "        label=\"micro-average ROC curve (area = {0:0.2f})\".format(roc_auc[\"micro\"]),\n",
    "        color=\"deeppink\",\n",
    "        linestyle=\":\",\n",
    "        linewidth=2,\n",
    "    )\n",
    "\n",
    "    ax[2].plot(\n",
    "        fpr[\"macro\"],\n",
    "        tpr[\"macro\"],\n",
    "        label=\"macro-average ROC curve (area = {0:0.2f})\".format(roc_auc[\"macro\"]),\n",
    "        color=\"navy\",\n",
    "        linestyle=\":\",\n",
    "        linewidth=2,\n",
    "    )\n",
    "    \n",
    "    lw= 1\n",
    "#     colors = cycle([\"aqua\", \"darkorange\", \"cornflowerblue\", \"green\"])\n",
    "    for i, color in zip(range(n_classes), colors):\n",
    "        ax[2].plot(\n",
    "            fpr[i],\n",
    "            tpr[i],\n",
    "            color=color,\n",
    "            lw= lw,\n",
    "            label=\"ROC curve of class {0} (area = {1:0.2f})\".format(i+1, roc_auc[i]),\n",
    "        )\n",
    "    \n",
    "    ax[2].plot([0, 1], [0, 1], \"k--\", lw= lw)\n",
    "    ax[2].set_xlim([-0.05, 0.5])\n",
    "    ax[2].set_ylim([0.5, 1.02])\n",
    "    ax[2].set_xlabel(\"False Positive Rate\")\n",
    "    ax[2].set_ylabel(\"True Positive Rate\")\n",
    "    ax[2].set_title(f'Magnified Roc-Curve ({modelTag})')\n",
    "    ax[2].legend(loc=\"lower right\")\n",
    "    \n",
    "    plt.savefig(f'{imageTag}.png', facecolor='w', bbox_inches=\"tight\",\n",
    "            pad_inches=0.3, transparent=True)\n",
    "    \n",
    "    plt.show()\n",
    "    return [n_classes, fpr, tpr, roc_auc, fpr['micro'], tpr['micro'], roc_auc['micro'],\n",
    "            fpr['macro'], tpr['macro'], roc_auc['macro'], precision[\"micro\"], recall[\"micro\"],\n",
    "            average_precision[\"micro\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ece39e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testRocPrecisionRecall(X, y, model, vectorizer, modelTag, imageTag):\n",
    "    new_Y04 = label_binarize(y, classes=[1, 2, 3, 4])\n",
    "    n_classes = new_Y04.shape[1]\n",
    "    X_train04, X_test04, y_train04, y_test04 = train_test_split(X, new_Y04, test_size=0.2, random_state= 1)\n",
    "    counts04= vectorizer\n",
    "    A04= counts04.fit_transform(X_train04.values.astype('str'), y_train04)\n",
    "    B04= counts04.transform(X_test04.values.astype('str'))\n",
    "    classifier = OneVsRestClassifier(model)\n",
    "    classifier.fit(A04, y_train04)\n",
    "    y_score= classifier.predict(B04)\n",
    "#     predictProbDict= classifier.predict_proba(B03)\n",
    "    predictProbDict= classifier.decision_function(B04)\n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(n_classes):\n",
    "    #     print(i)\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_test04[:, i], predictProbDict[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    \n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test04.ravel(), predictProbDict.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "    \n",
    "    # First aggregate all false positive rates\n",
    "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "    # Then interpolate all ROC curves at this points\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "    for i in range(n_classes):\n",
    "        mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "    # Finally average it and compute AUC\n",
    "    mean_tpr /= n_classes\n",
    "\n",
    "    fpr[\"macro\"] = all_fpr\n",
    "    tpr[\"macro\"] = mean_tpr\n",
    "    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "    \n",
    "    # Compute precision and recall area for each class\n",
    "    precision = dict()\n",
    "    recall = dict()\n",
    "    average_precision = dict()\n",
    "    for i in range(n_classes):\n",
    "        precision[i], recall[i], _ = precision_recall_curve(y_test04[:, i], predictProbDict[:, i])\n",
    "        average_precision[i] = average_precision_score(y_test04[:, i], predictProbDict[:, i])\n",
    "\n",
    "    # A \"micro-average\": quantifying score on all classes jointly\n",
    "    precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(y_test04.ravel(), predictProbDict.ravel()\n",
    "    )\n",
    "    average_precision[\"micro\"] = average_precision_score(y_test04, predictProbDict, average=\"micro\")\n",
    "\n",
    "    # plot\n",
    "    fig, ax= plt.subplots(2, 2, figsize= (16, 12))\n",
    "    fig.subplots_adjust(left= 0.02, right= 0.98, wspace= 0.2)\n",
    "    colors = cycle([\"aqua\", \"darkorange\", \"cornflowerblue\", \"green\"])\n",
    "    lw= 1\n",
    "    f_scores = np.linspace(0.2, 0.8, num=4)\n",
    "    lines, labels = [], []\n",
    "    for f_score in f_scores:\n",
    "        x = np.linspace(0.01, 1)\n",
    "        y = f_score * x / (2 * x - f_score)\n",
    "        (l,) = ax[0, 0].plot(x[y >= 0], y[y >= 0], color=\"gray\", alpha=0.2)\n",
    "        ax[0, 0].annotate(\"f1={0:0.1f}\".format(f_score), xy=(0.9, y[45] + 0.02))\n",
    "\n",
    "    display = PrecisionRecallDisplay(\n",
    "        recall=recall[\"micro\"],\n",
    "        precision=precision[\"micro\"],\n",
    "        average_precision=average_precision[\"micro\"],\n",
    "    )\n",
    "    display.plot(ax= ax[0, 0], name=\"Micro-average precision-recall\", color=\"deeppink\", lw= lw, linestyle=\":\")\n",
    "\n",
    "    for i, color in zip(range(n_classes), colors):\n",
    "        display = PrecisionRecallDisplay(\n",
    "            recall=recall[i],\n",
    "            precision=precision[i],\n",
    "            average_precision=average_precision[i],\n",
    "        )\n",
    "        display.plot(ax=ax[0, 0], name=f\"Precision-recall for class {i+1}\", color= color, lw= lw,)\n",
    "\n",
    "    # add the legend for the iso-f1 curves\n",
    "    handles, labels = display.ax_.get_legend_handles_labels()\n",
    "    handles.extend([l])\n",
    "    labels.extend([\"iso-f1 curves\"])\n",
    "    # set the legend and the axes\n",
    "    ax[0, 0].set_xlim([0.0, 1.0])\n",
    "    ax[0, 0].set_ylim([0.0, 1.05])\n",
    "    ax[0, 0].legend(handles=handles, labels=labels, loc=\"best\")\n",
    "    ax[0, 0].set_title(f\"Extension of Precision-Recall curve to multi-class ({modelTag})\")\n",
    "    \n",
    "#     Plot all ROC curves\n",
    "#     plt.figure(figsize=(20, 8))\n",
    "#     plt.figure(figsize=(10, 7))\n",
    "#     plt.subplot(121)\n",
    "    ax[1, 0].plot(\n",
    "        fpr[\"micro\"],\n",
    "        tpr[\"micro\"],\n",
    "        label=\"micro-average ROC curve (area = {0:0.2f})\".format(roc_auc[\"micro\"]),\n",
    "        color=\"deeppink\",\n",
    "        linestyle=\":\",\n",
    "        linewidth=2,\n",
    "    )\n",
    "\n",
    "    ax[1, 0].plot(\n",
    "        fpr[\"macro\"],\n",
    "        tpr[\"macro\"],\n",
    "        label=\"macro-average ROC curve (area = {0:0.2f})\".format(roc_auc[\"macro\"]),\n",
    "        color=\"navy\",\n",
    "        linestyle=\":\",\n",
    "        linewidth=2,\n",
    "    )\n",
    "    \n",
    "#     lw= 1\n",
    "#     colors = cycle([\"aqua\", \"darkorange\", \"cornflowerblue\", \"green\"])\n",
    "    for i, color in zip(range(n_classes), colors):\n",
    "        ax[1, 0].plot(\n",
    "            fpr[i],\n",
    "            tpr[i],\n",
    "            color=color,\n",
    "            lw= lw,\n",
    "            label=\"ROC curve of class {0} (area = {1:0.2f})\".format(i+1, roc_auc[i]),\n",
    "        )\n",
    "    \n",
    "    ax[1, 0].plot([0, 1], [0, 1], \"k--\", lw= lw)\n",
    "    ax[1, 0].set_xlim([-0.05, 1.0])\n",
    "    ax[1, 0].set_ylim([0.0, 1.05])\n",
    "    ax[1, 0].set_xlabel(\"False Positive Rate\")\n",
    "    ax[1, 0].set_ylabel(\"True Positive Rate\")\n",
    "    ax[1, 0].set_title(f\"Some extension of Receiver operating characteristic to multiclass ({modelTag})\")\n",
    "    ax[1, 0].legend(loc=\"lower right\")\n",
    "    \n",
    "#     plt.subplot(122)\n",
    "    ax[1, 1].plot(\n",
    "        fpr[\"micro\"],\n",
    "        tpr[\"micro\"],\n",
    "        label=\"micro-average ROC curve (area = {0:0.2f})\".format(roc_auc[\"micro\"]),\n",
    "        color=\"deeppink\",\n",
    "        linestyle=\":\",\n",
    "        linewidth=2,\n",
    "    )\n",
    "\n",
    "    ax[1, 1].plot(\n",
    "        fpr[\"macro\"],\n",
    "        tpr[\"macro\"],\n",
    "        label=\"macro-average ROC curve (area = {0:0.2f})\".format(roc_auc[\"macro\"]),\n",
    "        color=\"navy\",\n",
    "        linestyle=\":\",\n",
    "        linewidth=2,\n",
    "    )\n",
    "    \n",
    "#     lw= 1\n",
    "#     colors = cycle([\"aqua\", \"darkorange\", \"cornflowerblue\", \"green\"])\n",
    "    for i, color in zip(range(n_classes), colors):\n",
    "        ax[1, 1].plot(\n",
    "            fpr[i],\n",
    "            tpr[i],\n",
    "            color=color,\n",
    "            lw= lw,\n",
    "            label=\"ROC curve of class {0} (area = {1:0.2f})\".format(i+1, roc_auc[i]),\n",
    "        )\n",
    "    \n",
    "    ax[1, 1].plot([0, 1], [0, 1], \"k--\", lw= lw)\n",
    "    ax[1, 1].set_xlim([-0.05, 0.5])\n",
    "    ax[1, 1].set_ylim([0.5, 1.02])\n",
    "    ax[1, 1].set_xlabel(\"False Positive Rate\")\n",
    "    ax[1, 1].set_ylabel(\"True Positive Rate\")\n",
    "    ax[1, 1].set_title(f\"Magnified Roc-Curve ({modelTag})\")\n",
    "    ax[1, 1].legend(loc=\"lower right\")\n",
    "    \n",
    "    plt.savefig(f'{imageTag}.png', facecolor='w', bbox_inches=\"tight\",\n",
    "            pad_inches=0.3, transparent=True)\n",
    "    \n",
    "    plt.show()\n",
    "    return [n_classes, fpr, tpr, roc_auc, fpr['micro'], tpr['micro'], roc_auc['micro'],\n",
    "            fpr['macro'], tpr['macro'], roc_auc['macro'], precision[\"micro\"], recall[\"micro\"],\n",
    "            average_precision[\"micro\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d62b8a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcRocAuc(X, y, model, vectorizer, modelTag, imageTag):\n",
    "    new_Y03 = label_binarize(y, classes=[1, 2, 3, 4])\n",
    "    n_classes = new_Y03.shape[1]\n",
    "    X_train03, X_test03, y_train03, y_test03 = train_test_split(X, new_Y03, test_size=0.2, random_state= 1)\n",
    "    counts03= vectorizer\n",
    "    A03= counts03.fit_transform(X_train03.values.astype('str'), y_train03)\n",
    "    B03= counts03.transform(X_test03.values.astype('str'))\n",
    "    classifier = OneVsRestClassifier(model)\n",
    "    classifier.fit(A03, y_train03)\n",
    "    y_score= classifier.predict(B03)\n",
    "#     predictProbDict= classifier.predict_proba(B03)\n",
    "    predictProbDict= classifier.decision_function(B03)\n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(n_classes):\n",
    "    #     print(i)\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_test03[:, i], predictProbDict[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    \n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test03.ravel(), predictProbDict.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "    \n",
    "    # First aggregate all false positive rates\n",
    "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "    # Then interpolate all ROC curves at this points\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "    for i in range(n_classes):\n",
    "        mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "    # Finally average it and compute AUC\n",
    "    mean_tpr /= n_classes\n",
    "\n",
    "    fpr[\"macro\"] = all_fpr\n",
    "    tpr[\"macro\"] = mean_tpr\n",
    "    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "    # Plot all ROC curves\n",
    "    plt.figure(figsize=(20, 8))\n",
    "#     plt.figure(figsize=(10, 7))\n",
    "\n",
    "    plt.subplot(121)\n",
    "    plt.plot(\n",
    "        fpr[\"micro\"],\n",
    "        tpr[\"micro\"],\n",
    "        label=\"micro-average ROC curve (area = {0:0.2f})\".format(roc_auc[\"micro\"]),\n",
    "        color=\"deeppink\",\n",
    "        linestyle=\":\",\n",
    "        linewidth=2,\n",
    "    )\n",
    "\n",
    "    plt.plot(\n",
    "        fpr[\"macro\"],\n",
    "        tpr[\"macro\"],\n",
    "        label=\"macro-average ROC curve (area = {0:0.2f})\".format(roc_auc[\"macro\"]),\n",
    "        color=\"navy\",\n",
    "        linestyle=\":\",\n",
    "        linewidth=2,\n",
    "    )\n",
    "    \n",
    "    lw= 1\n",
    "    colors = cycle([\"aqua\", \"darkorange\", \"cornflowerblue\", \"green\"])\n",
    "    for i, color in zip(range(n_classes), colors):\n",
    "        plt.plot(\n",
    "            fpr[i],\n",
    "            tpr[i],\n",
    "            color=color,\n",
    "            lw= lw,\n",
    "            label=\"ROC curve of class {0} (area = {1:0.2f})\".format(i+1, roc_auc[i]),\n",
    "        )\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], \"k--\", lw= lw)\n",
    "    plt.xlim([-0.05, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(f\"Some extension of Receiver operating characteristic to multiclass ({modelTag})\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    plt.plot(\n",
    "        fpr[\"micro\"],\n",
    "        tpr[\"micro\"],\n",
    "        label=\"micro-average ROC curve (area = {0:0.2f})\".format(roc_auc[\"micro\"]),\n",
    "        color=\"deeppink\",\n",
    "        linestyle=\":\",\n",
    "        linewidth=2,\n",
    "    )\n",
    "\n",
    "    plt.plot(\n",
    "        fpr[\"macro\"],\n",
    "        tpr[\"macro\"],\n",
    "        label=\"macro-average ROC curve (area = {0:0.2f})\".format(roc_auc[\"macro\"]),\n",
    "        color=\"navy\",\n",
    "        linestyle=\":\",\n",
    "        linewidth=2,\n",
    "    )\n",
    "    \n",
    "    lw= 1\n",
    "    colors = cycle([\"aqua\", \"darkorange\", \"cornflowerblue\", \"green\"])\n",
    "    for i, color in zip(range(n_classes), colors):\n",
    "        plt.plot(\n",
    "            fpr[i],\n",
    "            tpr[i],\n",
    "            color=color,\n",
    "            lw= lw,\n",
    "            label=\"ROC curve of class {0} (area = {1:0.2f})\".format(i+1, roc_auc[i]),\n",
    "        )\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], \"k--\", lw= lw)\n",
    "    plt.xlim([-0.05, 0.5])\n",
    "    plt.ylim([0.5, 1.02])\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(f\"Magnified Roc-Curve ({modelTag})\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    \n",
    "    plt.savefig(f'{imageTag}.png', facecolor='w', bbox_inches=\"tight\",\n",
    "            pad_inches=0.3, transparent=True)\n",
    "    \n",
    "    plt.show()\n",
    "    return [n_classes, fpr, tpr, roc_auc, fpr['micro'], tpr['micro'], roc_auc['micro'],\n",
    "            fpr['macro'], tpr['macro'], roc_auc['macro']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
